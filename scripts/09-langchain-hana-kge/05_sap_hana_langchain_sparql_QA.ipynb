{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d4ad465-8e62-49b5-81c7-3e98d6e85e53",
   "metadata": {},
   "source": [
    "## Setup & Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06e4c74-5818-4347-9f3d-d63202ae68e5",
   "metadata": {},
   "source": [
    "**Prerequisite**:  \n",
    "You must have an SAP HANA Cloud instance with the **triple store** feature enabled.  \n",
    "For detailed instructions, refer to: [Enable Triple Store](https://help.sap.com/docs/hana-cloud-database/sap-hana-cloud-sap-hana-database-knowledge-graph-guide/enable-triple-store/)<br />\n",
    "Load the `kgdocu_movies` example data. See [Knowledge Graph Example](https://help.sap.com/docs/hana-cloud-database/sap-hana-cloud-sap-hana-database-knowledge-graph-guide/knowledge-graph-example)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c519a33-b21d-4a45-a6a9-b1538da40751",
   "metadata": {},
   "source": [
    "To use SAP HANA Knowledge Graph Engine and/or Vector Store Engine with LangChain, install the `langchain-hana` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8234ebee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain_hana\n",
    "print(langchain_hana.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37882055",
   "metadata": {},
   "source": [
    "## Connect to SAP HANA DB instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc3e3fb-9fad-464e-a68a-f49cb3da31bd",
   "metadata": {},
   "source": [
    "First, create a connection to your SAP HANA Cloud instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452fedb4-e121-41bd-92de-ae211d2bf928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from hdbcli import dbapi\n",
    "\n",
    "# Load environment variables if needed\n",
    "load_dotenv()\n",
    "\n",
    "# Establish connection to SAP HANA Cloud\n",
    "connection = dbapi.connect(\n",
    "    address=os.environ.get(\"HANADB_URL\"),\n",
    "    port=os.environ.get(\"HANADB_PRT\"),\n",
    "    user=os.environ.get(\"HANADB_USR\"),\n",
    "    password=os.environ.get(\"HANADB_PWD\"),\n",
    "    autocommit=True,\n",
    "    sslValidateCertificate=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cdf946",
   "metadata": {},
   "source": [
    "## Initialize the `HanaRdfGraph`\n",
    "\n",
    "To power the QA chain, you first need a `HanaRdfGraph` instance that:\n",
    "\n",
    "1. Loads your ontology schema (in Turtle)  \n",
    "2. Executes SPARQL queries against your SAP HANA Cloud data graph  \n",
    "\n",
    "The constructor requires:\n",
    "\n",
    "- **`connection`**: an active `hdbcli.dbapi.connect(...)` instance  \n",
    "- **`graph_uri`**: the named graph (or `\"DEFAULT\"`) where your RDF data lives  \n",
    "- **One of**:  \n",
    "  1. `ontology_query`**: a SPARQL CONSTRUCT to extract schema triples  \n",
    "  2. `ontology_uri`**: a hosted ontology graph URI  \n",
    "  3. `ontology_local_file`** + **`ontology_local_file_format`**: a local Turtle/RDF file  \n",
    "  4. `auto_extract_ontology=True`** (not recommended for production—see note)\n",
    "\n",
    "`graph_uri` vs. Ontology\n",
    "- **`graph_uri`**:  \n",
    "  The named graph in your SAP HANA Cloud instance that contains your instance data (sometimes 100k+ triples).\n",
    "  If `None` or `\"DEFAULT\"` is provided, the default graph is used.  \n",
    "  ➔ More details: [Default Graph and Named Graphs](https://help.sap.com/docs/hana-cloud-database/sap-hana-cloud-sap-hana-database-knowledge-graph-guide/default-graph-and-named-graphs)\n",
    "- **Ontology**: a lean schema (typically ~50-100 triples) describing classes, properties, domains, ranges, labels, comments, and subclass relationships. The ontology guides SPARQL generation and result interpretation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e51f87a-ffe1-464b-874e-297fe5dd4547",
   "metadata": {},
   "source": [
    "## Example: Question Answering over a “Movies” Knowledge Graph\n",
    "\n",
    "Below we’ll:\n",
    "\n",
    "1. Instantiate the `HanaRdfGraph` pointing at our “movies” data graph  \n",
    "2. Wrap it in a `HanaSparqlQAChain` powered by an LLM  \n",
    "3. Ask natural-language questions and print out the chain’s responses  \n",
    "\n",
    "This demonstrates how the LLM generates SPARQL under the hood, executes it against SAP HANA Cloud, and returns a human-readable answer.\n",
    "\n",
    "We'll use the `sap-ai-sdk-gen` package. Check [sap-ai-sdk-gen](https://pypi.org/project/sap-ai-sdk-gen/) for futher details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adba3dd8-2786-4157-9db5-901ffa3d89d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_hana import HanaRdfGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2e0e4c-e242-42f5-b117-83ce9db0e3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Knowledge Graph\n",
    "graph_uri = \"kgdocu_movies\"\n",
    "\n",
    "graph = HanaRdfGraph(\n",
    "    connection=connection, graph_uri=graph_uri, auto_extract_ontology=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daab00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a basic graph schema is extracted from the data graph. This schema will guide the LLM to generate a proper SPARQL query.\n",
    "print(graph.get_schema)\n",
    "schema_ttl = graph.get_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0a0d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "from rdflib.tools.rdf2dot import rdf2dot\n",
    "import io\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0063f2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fbb66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse into an RDFLib graph\n",
    "g = rdflib.Graph()\n",
    "g.parse(data=graph.get_schema, format=\"turtle\")\n",
    "\n",
    "# Build a NetworkX graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add edges based on domain-range relationships\n",
    "for prop in g.subjects(rdflib.RDF.type, rdflib.OWL.ObjectProperty):\n",
    "    domain = g.value(prop, rdflib.RDFS.domain)\n",
    "    range_ = g.value(prop, rdflib.RDFS.range)\n",
    "    label = g.value(prop, rdflib.RDFS.label) or prop.split('/')[-1]\n",
    "    if domain and range_:\n",
    "        d_label = domain.split('/')[-1]\n",
    "        r_label = range_.split('/')[-1]\n",
    "        G.add_node(d_label)\n",
    "        G.add_node(r_label)\n",
    "        G.add_edge(d_label, r_label, label=str(label))\n",
    "\n",
    "for prop in g.subjects(rdflib.RDF.type, rdflib.OWL.DatatypeProperty):\n",
    "    domain = g.value(prop, rdflib.RDFS.domain)\n",
    "    range_ = g.value(prop, rdflib.RDFS.range)\n",
    "    label = g.value(prop, rdflib.RDFS.label) or prop.split('/')[-1]\n",
    "    if domain and range_:\n",
    "        d_label = domain.split('/')[-1]\n",
    "        r_label = range_.split('/')[-1]\n",
    "        G.add_node(d_label)\n",
    "        G.add_node(r_label)\n",
    "        G.add_edge(d_label, r_label, label=str(label))\n",
    "\n",
    "# Draw using Matplotlib\n",
    "# pos = nx.spring_layout(G)\n",
    "# pos = nx.kamada_kawai_layout(G, weight=None)\n",
    "pos = nx.circular_layout(G)\n",
    "# pos = nx.spectral_layout(G)\n",
    "\n",
    "nx.draw(G, pos, with_labels=True, node_size=2000)\n",
    "edge_labels = nx.get_edge_attributes(G, 'label')\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)\n",
    "plt.title(\"Ontology Schema Graph\")\n",
    "# plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d79e0f",
   "metadata": {},
   "source": [
    "## Executing SPARQL Queries\n",
    "\n",
    "You can use the `query()` method to execute arbitrary SPARQL queries (`SELECT`, `ASK`, `CONSTRUCT`, etc.) on the data graph.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e944500",
   "metadata": {},
   "source": [
    "The following query retrieves the top 10 movies with the highest number of contributors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a266c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "PREFIX kg: <http://kg.demo.sap.com/>\n",
    "SELECT ?movieTitle (COUNT(?actor) AS ?actorCount)\n",
    "\n",
    "FROM <kgdocu_movies>\n",
    "WHERE {\n",
    "    ?actor kg:acted_in ?movie .\n",
    "    ?movie kg:title ?movieTitle .\n",
    "}\n",
    "GROUP BY ?movieTitle\n",
    "ORDER BY DESC(?actorCount)\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "top10 = graph.query(query)\n",
    "pd.read_csv(io.StringIO(top10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d20d3cd",
   "metadata": {},
   "source": [
    "## Question Answering with `HanaSparqlQAChain`\n",
    "\n",
    "`HanaSparqlQAChain` ties together:\n",
    "\n",
    "1. **Schema-aware SPARQL generation**  \n",
    "2. **Query execution** against SAP HANA  \n",
    "3. **Natural-language answer formatting**\n",
    "\n",
    "\n",
    "### Initialization\n",
    "\n",
    "You need:\n",
    "\n",
    "- An **LLM** to generate and interpret queries  \n",
    "- A **`HanaRdfGraph`** (with connection, `graph_uri`, and ontology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a5d210",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_hana import HanaSparqlQAChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e5b83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.proxy.langchain.openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58cabc8-3912-49d2-9296-e8d49cc3525c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(proxy_model_name=\"gpt-4.1\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29a141d-f511-4930-8ea2-51d3b02bb5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SPARQL QA Chain\n",
    "chain = HanaSparqlQAChain.from_llm(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    allow_dangerous_requests=True,\n",
    "    graph=graph,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f3ebe8-8ea5-4e26-b311-73c7682d3fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = chain.invoke(\"Which movies are in the data?\")\n",
    "# output = chain.invoke(\"In which movies did Keanu Reeves and Carrie-Anne Moss play in together.\")\n",
    "output = chain.invoke(\"which movie genres are in the data?\")\n",
    "# output = chain.invoke(\"which are the two most assigned movie genres?\")\n",
    "# output = chain.invoke('where were the actors of \"Blade Runner\" born?')\n",
    "# output = chain.invoke(\"which actors acted together in a movie and were born in the same city?\")\n",
    "# output = chain.invoke(\"which actors acted in Blade Runner?\")\n",
    "\n",
    "display(Markdown(output[\"result\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40a3f21-e26a-4a9e-9f1f-47435db3d14d",
   "metadata": {},
   "source": [
    "### What’s happening under the hood?\n",
    "\n",
    "1. **SPARQL Generation**  \n",
    "   The chain invokes the LLM with your Turtle-formatted ontology (`graph.get_schema`) and the user’s question using the `SPARQL_GENERATION_SELECT_PROMPT`. The LLM then emits a valid `SELECT` query tailored to your schema.\n",
    "\n",
    "2. **Pre-processing & Execution**  \n",
    "   - **Extract & clean**: Pull the raw SPARQL text out of the LLM’s response.  \n",
    "   - **Inject graph context**: Add `FROM <graph_uri>` if it’s missing and ensure common prefixes (`rdf:`, `rdfs:`, `owl:`, `xsd:`) are declared.  \n",
    "   - **Run on HANA**: Execute the finalized query via `HanaRdfGraph.query()` over your named graph.\n",
    "\n",
    "3. **Answer Formulation**  \n",
    "   The returned CSV (or Turtle) results feed into the LLM again—this time with the `SPARQL_QA_PROMPT`. The LLM produces a concise, human-readable answer strictly based on the retrieved data, without hallucination.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ab3fc0-40af-474d-adf7-e42078b19a35",
   "metadata": {},
   "source": [
    "### Pipeline Overview\n",
    "\n",
    "1. **SPARQL Generation**  \n",
    "   - Uses `SPARQL_GENERATION_SELECT_PROMPT`  \n",
    "   - Inputs:  \n",
    "     - `schema` (Turtle from `graph.get_schema`)  \n",
    "     - `prompt` (user’s question)  \n",
    "2. **Query Post-processing**  \n",
    "   - Extracts the SPARQL code from the llm output.\n",
    "   - Inject `FROM <graph_uri>` if missing  \n",
    "   - Ensure required common prefixes are declared (`rdf:`, `rdfs:`, `owl:`, `xsd:`)  \n",
    "3. **Execution**  \n",
    "   - Calls `graph.query(generated_sparql)`  \n",
    "4. **Answer Formulation**  \n",
    "   - Uses `SPARQL_QA_PROMPT`  \n",
    "   - Inputs:  \n",
    "     - `context` (raw query results)  \n",
    "     - `prompt` (original question)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c014af-71a8-4ef8-8a54-2df0d7edb8e5",
   "metadata": {},
   "source": [
    "### Prompt Templates\n",
    "\n",
    "\n",
    "#### \"SPARQL Generation\" prompt\n",
    "\n",
    "The `sparql_generation_prompt` is used to guide the LLM in generating a SPARQL query from the user question and the provided schema.\n",
    "\n",
    "#### Answering prompt\n",
    "\n",
    "The `qa_prompt` instructs the LLM to create a natural language answer based solely on the database results.\n",
    "\n",
    "The default prompts can be found here: [`prompts.py`](https://github.com/SAP/langchain-integration-for-sap-hana-cloud/blob/main/langchain_hana/chains/graph_qa/prompts.py)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
